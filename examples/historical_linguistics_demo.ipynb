{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Linguistics with AlteruPhono: Complete Workflow Demo\n",
    "\n",
    "This notebook demonstrates advanced historical linguistics workflows using AlteruPhono, including:\n",
    "- Comparative reconstruction\n",
    "- Language evolution simulation\n",
    "- Phonological distance analysis\n",
    "- Sound change rule discovery\n",
    "- Cross-linguistic typology\n",
    "\n",
    "**Prerequisites**: Basic knowledge of historical linguistics and phonology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "import alteruphono\n",
    "from alteruphono.phonology.sound_change import (\n",
    "    SoundChangeEngine, \n",
    "    SoundChangeRule, \n",
    "    FeatureChangeRule,\n",
    "    RuleSet\n",
    ")\n",
    "from alteruphono.phonology.sound_change.rules import FeatureChange, ChangeType\n",
    "from alteruphono.phonology.sound_change.environments import PhonologicalEnvironment\n",
    "from alteruphono.phonology.sound_v2 import Sound\n",
    "from alteruphono.phonology.feature_systems import get_feature_system, convert_between_systems\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"AlteruPhono Historical Linguistics Demo\")\n",
    "print(\"=====================================\")\n",
    "print(f\"AlteruPhono version: {alteruphono.__version__ if hasattr(alteruphono, '__version__') else 'development'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparative Reconstruction Workflow\n",
    "\n",
    "We'll reconstruct a proto-language from descendant forms using systematic correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cognate sets from related languages\n",
    "cognate_sets = {\n",
    "    'water': {\n",
    "        'Language_A': ['w', 'a', 't', 'a'],\n",
    "        'Language_B': ['b', 'a', 'd', 'a'],  \n",
    "        'Language_C': ['v', 'a', 't', 'a']\n",
    "    },\n",
    "    'fire': {\n",
    "        'Language_A': ['f', 'i', 'r', 'a'],\n",
    "        'Language_B': ['p', 'i', 'r', 'a'],\n",
    "        'Language_C': ['f', 'i', 'r', 'a']\n",
    "    },\n",
    "    'stone': {\n",
    "        'Language_A': ['s', 't', 'o', 'n'],\n",
    "        'Language_B': ['s', 't', 'o', 'n'],\n",
    "        'Language_C': ['ʃ', 't', 'o', 'n']\n",
    "    },\n",
    "    'path': {\n",
    "        'Language_A': ['p', 'a', 'θ'],\n",
    "        'Language_B': ['p', 'a', 't'],\n",
    "        'Language_C': ['p', 'a', 't']\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_correspondences(cognate_sets):\n",
    "    \"\"\"Extract systematic sound correspondences from cognate sets.\"\"\"\n",
    "    correspondences = defaultdict(list)\n",
    "    \n",
    "    for meaning, languages in cognate_sets.items():\n",
    "        # Get maximum length for alignment\n",
    "        max_len = max(len(word) for word in languages.values())\n",
    "        \n",
    "        # Align cognates (simple position-based alignment)\n",
    "        for pos in range(max_len):\n",
    "            sounds_at_pos = []\n",
    "            for lang, word in languages.items():\n",
    "                if pos < len(word):\n",
    "                    sounds_at_pos.append((lang, word[pos]))\n",
    "            \n",
    "            if len(sounds_at_pos) >= 2:  # Need at least 2 languages\n",
    "                # Create correspondence tuple\n",
    "                correspondence = tuple(sound for lang, sound in sorted(sounds_at_pos))\n",
    "                correspondences[correspondence].append((meaning, pos))\n",
    "    \n",
    "    return correspondences\n",
    "\n",
    "correspondences = extract_correspondences(cognate_sets)\n",
    "\n",
    "print(\"Systematic Sound Correspondences:\")\n",
    "print(\"=\" * 40)\n",
    "for corr, contexts in correspondences.items():\n",
    "    if len(contexts) > 1:  # Only show recurrent correspondences\n",
    "        lang_a, lang_b, lang_c = corr\n",
    "        context_str = ', '.join(f\"{meaning}:{pos}\" for meaning, pos in contexts)\n",
    "        print(f\"A:{lang_a} ~ B:{lang_b} ~ C:{lang_c} ({context_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct proto-forms using the comparative method\n",
    "def reconstruct_proto_form(correspondence_set):\n",
    "    \"\"\"Reconstruct proto-sound from correspondence set.\"\"\"\n",
    "    # Simple reconstruction heuristics\n",
    "    sounds = set(correspondence_set)\n",
    "    \n",
    "    # If all languages agree, reconstruct the common sound\n",
    "    if len(sounds) == 1:\n",
    "        return list(sounds)[0]\n",
    "    \n",
    "    # Apply reconstruction principles\n",
    "    # Example: voiceless stops are often more conservative\n",
    "    voiceless_stops = {'p', 't', 'k'}\n",
    "    for sound in sounds:\n",
    "        if sound in voiceless_stops:\n",
    "            return sound\n",
    "    \n",
    "    # Default: take the most common sound\n",
    "    from collections import Counter\n",
    "    return Counter(correspondence_set).most_common(1)[0][0]\n",
    "\n",
    "def reconstruct_proto_language(cognate_sets):\n",
    "    \"\"\"Reconstruct proto-forms for all cognate sets.\"\"\"\n",
    "    proto_language = {}\n",
    "    \n",
    "    for meaning, languages in cognate_sets.items():\n",
    "        # Get all language forms\n",
    "        forms = list(languages.values())\n",
    "        max_len = max(len(form) for form in forms)\n",
    "        \n",
    "        proto_form = []\n",
    "        for pos in range(max_len):\n",
    "            sounds_at_pos = []\n",
    "            for form in forms:\n",
    "                if pos < len(form):\n",
    "                    sounds_at_pos.append(form[pos])\n",
    "            \n",
    "            if sounds_at_pos:\n",
    "                proto_sound = reconstruct_proto_form(sounds_at_pos)\n",
    "                proto_form.append(proto_sound)\n",
    "        \n",
    "        proto_language[meaning] = proto_form\n",
    "    \n",
    "    return proto_language\n",
    "\n",
    "proto_forms = reconstruct_proto_language(cognate_sets)\n",
    "\n",
    "print(\"\\nProto-Language Reconstruction:\")\n",
    "print(\"=\" * 35)\n",
    "for meaning, proto_form in proto_forms.items():\n",
    "    proto_str = ' '.join(proto_form)\n",
    "    print(f\"*{proto_str} '{meaning}'\")\n",
    "    \n",
    "    # Show reflexes in daughter languages\n",
    "    for lang, form in cognate_sets[meaning].items():\n",
    "        form_str = ' '.join(form)\n",
    "        print(f\"  {lang}: {form_str}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Evolution Simulation\n",
    "\n",
    "Simulate the evolution of the reconstructed proto-language into the attested daughter languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sound change rules for each language branch\n",
    "engine = SoundChangeEngine(feature_system_name='unified_distinctive')\n",
    "\n",
    "# Language A changes: relatively conservative\n",
    "lang_a_rules = [\n",
    "    # θ > θ (no change)\n",
    "    SoundChangeRule(name=\"a_conservative\", source_pattern=\"θ\", target_pattern=\"θ\")\n",
    "]\n",
    "\n",
    "# Language B changes: more innovative\n",
    "lang_b_rules = [\n",
    "    # f > p (initial strengthening)\n",
    "    SoundChangeRule(name=\"b_f_to_p\", source_pattern=\"f\", target_pattern=\"p\", environment=\"# _\"),\n",
    "    # w > b (fortition)\n",
    "    SoundChangeRule(name=\"b_w_to_b\", source_pattern=\"w\", target_pattern=\"b\"),\n",
    "    # t > d / V_V (intervocalic voicing)\n",
    "    SoundChangeRule(name=\"b_t_to_d\", source_pattern=\"t\", target_pattern=\"d\", environment=\"V _ V\"),\n",
    "    # θ > t (stopping)\n",
    "    SoundChangeRule(name=\"b_theta_to_t\", source_pattern=\"θ\", target_pattern=\"t\")\n",
    "]\n",
    "\n",
    "# Language C changes: moderate innovation\n",
    "lang_c_rules = [\n",
    "    # w > v (partial fortition)\n",
    "    SoundChangeRule(name=\"c_w_to_v\", source_pattern=\"w\", target_pattern=\"v\"),\n",
    "    # s > ʃ / _ t (assimilation)\n",
    "    SoundChangeRule(name=\"c_s_to_sh\", source_pattern=\"s\", target_pattern=\"ʃ\", environment=\"_ t\"),\n",
    "    # θ > t (stopping)\n",
    "    SoundChangeRule(name=\"c_theta_to_t\", source_pattern=\"θ\", target_pattern=\"t\")\n",
    "]\n",
    "\n",
    "# Create rule sets\n",
    "lang_a_evolution = RuleSet(rules=lang_a_rules, iterative=False)\n",
    "lang_b_evolution = RuleSet(rules=lang_b_rules, iterative=False)\n",
    "lang_c_evolution = RuleSet(rules=lang_c_rules, iterative=False)\n",
    "\n",
    "def simulate_evolution(proto_forms, rule_sets):\n",
    "    \"\"\"Simulate evolution of proto-language into daughter languages.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for lang_name, rules in rule_sets.items():\n",
    "        results[lang_name] = {}\n",
    "        \n",
    "        for meaning, proto_form in proto_forms.items():\n",
    "            # Convert to Sound objects\n",
    "            sounds = [Sound(s, 'unified_distinctive') for s in proto_form]\n",
    "            \n",
    "            # Apply evolution rules\n",
    "            result = engine.apply_rule_set(rules, sounds)\n",
    "            \n",
    "            # Extract final form\n",
    "            evolved_form = [s.grapheme() for s in result.final_sequence]\n",
    "            results[lang_name][meaning] = evolved_form\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate evolution\n",
    "rule_sets = {\n",
    "    'Simulated_A': lang_a_evolution,\n",
    "    'Simulated_B': lang_b_evolution,\n",
    "    'Simulated_C': lang_c_evolution\n",
    "}\n",
    "\n",
    "simulated_results = simulate_evolution(proto_forms, rule_sets)\n",
    "\n",
    "print(\"Language Evolution Simulation:\")\n",
    "print(\"=\" * 35)\n",
    "for meaning in proto_forms.keys():\n",
    "    proto_str = ' '.join(proto_forms[meaning])\n",
    "    print(f\"*{proto_str} '{meaning}'\")\n",
    "    \n",
    "    # Show simulated vs. actual forms\n",
    "    for lang in ['A', 'B', 'C']:\n",
    "        actual_form = ' '.join(cognate_sets[meaning][f'Language_{lang}'])\n",
    "        simulated_form = ' '.join(simulated_results[f'Simulated_{lang}'][meaning])\n",
    "        match = \"✓\" if actual_form == simulated_form else \"✗\"\n",
    "        print(f\"  {lang}: {actual_form} (simulated: {simulated_form}) {match}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phonological Distance Analysis\n",
    "\n",
    "Calculate phonological distances between languages for classification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexical_distance(lang1_forms, lang2_forms, feature_system='tresoldi_distinctive'):\n",
    "    \"\"\"Calculate average phonological distance between two languages.\"\"\"\n",
    "    total_distance = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    for meaning in lang1_forms.keys():\n",
    "        if meaning in lang2_forms:\n",
    "            word1 = lang1_forms[meaning]\n",
    "            word2 = lang2_forms[meaning]\n",
    "            \n",
    "            # Calculate word-level distance\n",
    "            word_distance = calculate_word_distance(word1, word2, feature_system)\n",
    "            total_distance += word_distance\n",
    "            word_count += 1\n",
    "    \n",
    "    return total_distance / word_count if word_count > 0 else 0\n",
    "\n",
    "def calculate_word_distance(word1, word2, feature_system='tresoldi_distinctive'):\n",
    "    \"\"\"Calculate phonological distance between two words.\"\"\"\n",
    "    # Convert to Sound objects\n",
    "    sounds1 = [Sound(s, feature_system) for s in word1]\n",
    "    sounds2 = [Sound(s, feature_system) for s in word2]\n",
    "    \n",
    "    # Align words (simple approach: pad shorter word)\n",
    "    max_len = max(len(sounds1), len(sounds2))\n",
    "    \n",
    "    # Calculate segment-by-segment distances\n",
    "    total_distance = 0\n",
    "    for i in range(max_len):\n",
    "        if i < len(sounds1) and i < len(sounds2):\n",
    "            # Both have sounds at this position\n",
    "            distance = sounds1[i].distance_to(sounds2[i])\n",
    "            total_distance += distance\n",
    "        else:\n",
    "            # One word is shorter - penalize insertion/deletion\n",
    "            total_distance += 1.0  # Maximum distance penalty\n",
    "    \n",
    "    return total_distance / max_len  # Normalize by length\n",
    "\n",
    "# Calculate distance matrix\n",
    "languages = ['Language_A', 'Language_B', 'Language_C']\n",
    "distance_matrix = np.zeros((len(languages), len(languages)))\n",
    "\n",
    "for i, lang1 in enumerate(languages):\n",
    "    for j, lang2 in enumerate(languages):\n",
    "        if i != j:\n",
    "            # Extract language-specific forms\n",
    "            lang1_forms = {meaning: forms[lang1] for meaning, forms in cognate_sets.items()}\n",
    "            lang2_forms = {meaning: forms[lang2] for meaning, forms in cognate_sets.items()}\n",
    "            \n",
    "            distance = calculate_lexical_distance(lang1_forms, lang2_forms)\n",
    "            distance_matrix[i][j] = distance\n",
    "\n",
    "# Create distance DataFrame\n",
    "distance_df = pd.DataFrame(distance_matrix, index=languages, columns=languages)\n",
    "\n",
    "print(\"Phonological Distance Matrix:\")\n",
    "print(\"=\" * 35)\n",
    "print(distance_df.round(3))\n",
    "\n",
    "# Visualize distance matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(distance_df, annot=True, cmap='viridis_r', square=True, \n",
    "            fmt='.3f', cbar_kws={'label': 'Phonological Distance'})\n",
    "plt.title('Phonological Distance Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build phylogenetic tree from distance matrix\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Convert to condensed distance matrix\n",
    "condensed_distances = squareform(distance_matrix)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(condensed_distances, method='average')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, labels=languages, leaf_rotation=45)\n",
    "plt.title('Phonological Distance-Based Language Tree')\n",
    "plt.xlabel('Languages')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze clustering results\n",
    "print(\"\\nPhylogenetic Analysis:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Closest languages: {languages[np.unravel_index(np.argmin(distance_matrix + np.eye(len(languages))*999), distance_matrix.shape)]}\")\n",
    "print(f\"Most distant languages: {languages[np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sound Change Rule Discovery\n",
    "\n",
    "Automatically discover sound change rules from comparative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_sound_changes(proto_forms, daughter_forms):\n",
    "    \"\"\"Automatically discover sound change rules from comparative data.\"\"\"\n",
    "    discovered_rules = []\n",
    "    \n",
    "    # Collect all sound changes\n",
    "    changes = defaultdict(list)\n",
    "    \n",
    "    for meaning in proto_forms.keys():\n",
    "        if meaning in daughter_forms:\n",
    "            proto_word = proto_forms[meaning]\n",
    "            daughter_word = daughter_forms[meaning]\n",
    "            \n",
    "            # Align words and find changes\n",
    "            max_len = max(len(proto_word), len(daughter_word))\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                proto_sound = proto_word[i] if i < len(proto_word) else None\n",
    "                daughter_sound = daughter_word[i] if i < len(daughter_word) else None\n",
    "                \n",
    "                if proto_sound and daughter_sound and proto_sound != daughter_sound:\n",
    "                    # Extract context\n",
    "                    left_context = proto_word[i-1] if i > 0 else '#'\n",
    "                    right_context = proto_word[i+1] if i < len(proto_word)-1 else '#'\n",
    "                    \n",
    "                    change_key = (proto_sound, daughter_sound)\n",
    "                    context = (left_context, right_context)\n",
    "                    changes[change_key].append(context)\n",
    "    \n",
    "    # Analyze patterns\n",
    "    for (source, target), contexts in changes.items():\n",
    "        if len(contexts) >= 2:  # Need multiple instances\n",
    "            # Find common contexts\n",
    "            left_contexts = [ctx[0] for ctx in contexts]\n",
    "            right_contexts = [ctx[1] for ctx in contexts]\n",
    "            \n",
    "            # Simple rule discovery: find most common context\n",
    "            from collections import Counter\n",
    "            common_left = Counter(left_contexts).most_common(1)[0][0]\n",
    "            common_right = Counter(right_contexts).most_common(1)[0][0]\n",
    "            \n",
    "            # Create rule description\n",
    "            if common_left == '#' and common_right == '#':\n",
    "                environment = \"everywhere\"\n",
    "            elif common_left == '#':\n",
    "                environment = f\"word-initially (before {common_right})\"\n",
    "            elif common_right == '#':\n",
    "                environment = f\"word-finally (after {common_left})\"\n",
    "            else:\n",
    "                environment = f\"after {common_left}, before {common_right}\"\n",
    "            \n",
    "            rule = {\n",
    "                'source': source,\n",
    "                'target': target,\n",
    "                'environment': environment,\n",
    "                'frequency': len(contexts),\n",
    "                'examples': contexts[:3]  # Show first 3 examples\n",
    "            }\n",
    "            \n",
    "            discovered_rules.append(rule)\n",
    "    \n",
    "    return discovered_rules\n",
    "\n",
    "# Discover rules for Language B\n",
    "lang_b_forms = {meaning: forms['Language_B'] for meaning, forms in cognate_sets.items()}\n",
    "lang_b_rules = discover_sound_changes(proto_forms, lang_b_forms)\n",
    "\n",
    "print(\"Discovered Sound Changes (Proto → Language B):\")\n",
    "print(\"=\" * 50)\n",
    "for rule in sorted(lang_b_rules, key=lambda x: x['frequency'], reverse=True):\n",
    "    print(f\"{rule['source']} → {rule['target']} / {rule['environment']}\")\n",
    "    print(f\"  Frequency: {rule['frequency']} instances\")\n",
    "    print(f\"  Examples: {rule['examples'][:2]}\")\n",
    "    print()\n",
    "\n",
    "# Discover rules for Language C\n",
    "lang_c_forms = {meaning: forms['Language_C'] for meaning, forms in cognate_sets.items()}\n",
    "lang_c_rules = discover_sound_changes(proto_forms, lang_c_forms)\n",
    "\n",
    "print(\"Discovered Sound Changes (Proto → Language C):\")\n",
    "print(\"=\" * 50)\n",
    "for rule in sorted(lang_c_rules, key=lambda x: x['frequency'], reverse=True):\n",
    "    print(f\"{rule['source']} → {rule['target']} / {rule['environment']}\")\n",
    "    print(f\"  Frequency: {rule['frequency']} instances\")\n",
    "    print(f\"  Examples: {rule['examples'][:2]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Linguistic Phonological Analysis\n",
    "\n",
    "Analyze phonological inventories and typological patterns using the Tresoldi comprehensive system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phonological inventories for different language types\n",
    "language_inventories = {\n",
    "    'Minimal_System': {\n",
    "        'consonants': ['p', 't', 'k', 'm', 'n', 's'],\n",
    "        'vowels': ['a', 'i', 'u']\n",
    "    },\n",
    "    'Average_System': {\n",
    "        'consonants': ['p', 'b', 't', 'd', 'k', 'g', 'm', 'n', 'ŋ', 'f', 's', 'ʃ', 'l', 'r'],\n",
    "        'vowels': ['a', 'e', 'i', 'o', 'u']\n",
    "    },\n",
    "    'Complex_System': {\n",
    "        'consonants': ['p', 'b', 't', 'd', 'k', 'g', 'q', 'ʔ', 'm', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'ʒ', 'x', 'ɣ', 'h', 'l', 'r', 'j', 'w'],\n",
    "        'vowels': ['a', 'æ', 'e', 'ɛ', 'i', 'ɪ', 'o', 'ɔ', 'u', 'ʊ', 'ə']\n",
    "    },\n",
    "    'Click_System': {\n",
    "        'consonants': ['p', 't', 'k', 'm', 'n', 's', 'l', 'ǀ', 'ǁ', 'ǃ'],  # Includes clicks\n",
    "        'vowels': ['a', 'e', 'i', 'o', 'u']\n",
    "    }\n",
    "}\n",
    "\n",
    "def analyze_inventory_typology(inventory, system_name='tresoldi_distinctive'):\n",
    "    \"\"\"Analyze typological properties of a phonological inventory.\"\"\"\n",
    "    tresoldi_system = get_feature_system(system_name)\n",
    "    \n",
    "    analysis = {\n",
    "        'total_sounds': 0,\n",
    "        'consonants': 0,\n",
    "        'vowels': 0,\n",
    "        'places_of_articulation': set(),\n",
    "        'manners_of_articulation': set(),\n",
    "        'rare_features': defaultdict(int),\n",
    "        'feature_complexity': 0\n",
    "    }\n",
    "    \n",
    "    all_sounds = inventory['consonants'] + inventory['vowels']\n",
    "    analysis['total_sounds'] = len(all_sounds)\n",
    "    \n",
    "    rare_features = ['click', 'ejective', 'implosive', 'creaky', 'breathy']\n",
    "    \n",
    "    for sound_char in all_sounds:\n",
    "        if tresoldi_system.has_grapheme(sound_char):\n",
    "            sound = Sound(sound_char, system_name)\n",
    "            \n",
    "            # Count major classes\n",
    "            if sound.get_feature_value('consonantal') > 0:\n",
    "                analysis['consonants'] += 1\n",
    "            if sound.get_feature_value('syllabic') > 0:\n",
    "                analysis['vowels'] += 1\n",
    "            \n",
    "            # Collect places of articulation\n",
    "            place_features = ['labial', 'coronal', 'dorsal', 'glottal']\n",
    "            for place in place_features:\n",
    "                if sound.has_feature(place) and sound.get_feature_value(place) > 0:\n",
    "                    analysis['places_of_articulation'].add(place)\n",
    "            \n",
    "            # Collect manners of articulation  \n",
    "            manner_features = ['nasal', 'continuant', 'lateral', 'strident']\n",
    "            for manner in manner_features:\n",
    "                if sound.has_feature(manner) and sound.get_feature_value(manner) > 0:\n",
    "                    analysis['manners_of_articulation'].add(manner)\n",
    "            \n",
    "            # Count rare features\n",
    "            for rare_feat in rare_features:\n",
    "                if sound.has_feature(rare_feat) and sound.get_feature_value(rare_feat) > 0:\n",
    "                    analysis['rare_features'][rare_feat] += 1\n",
    "            \n",
    "            # Calculate feature complexity (number of active features)\n",
    "            active_features = sum(1 for feat in sound.features.features if abs(feat.value) > 0.1)\n",
    "            analysis['feature_complexity'] += active_features\n",
    "    \n",
    "    # Calculate average complexity\n",
    "    if analysis['total_sounds'] > 0:\n",
    "        analysis['avg_feature_complexity'] = analysis['feature_complexity'] / analysis['total_sounds']\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Analyze all inventories\n",
    "typological_analyses = {}\n",
    "for lang_type, inventory in language_inventories.items():\n",
    "    analysis = analyze_inventory_typology(inventory)\n",
    "    typological_analyses[lang_type] = analysis\n",
    "\n",
    "# Create comparative table\n",
    "comparison_data = []\n",
    "for lang_type, analysis in typological_analyses.items():\n",
    "    row = {\n",
    "        'Language_Type': lang_type,\n",
    "        'Total_Sounds': analysis['total_sounds'],\n",
    "        'Consonants': analysis['consonants'],\n",
    "        'Vowels': analysis['vowels'],\n",
    "        'C/V_Ratio': analysis['consonants'] / max(analysis['vowels'], 1),\n",
    "        'Places': len(analysis['places_of_articulation']),\n",
    "        'Manners': len(analysis['manners_of_articulation']),\n",
    "        'Avg_Complexity': analysis.get('avg_feature_complexity', 0),\n",
    "        'Rare_Features': sum(analysis['rare_features'].values())\n",
    "    }\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(2)\n",
    "\n",
    "print(\"Cross-Linguistic Typological Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize typological patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Inventory size distribution\n",
    "axes[0,0].bar(comparison_df['Language_Type'], comparison_df['Total_Sounds'], color='skyblue')\n",
    "axes[0,0].set_title('Total Inventory Size')\n",
    "axes[0,0].set_ylabel('Number of Sounds')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: C/V ratio\n",
    "axes[0,1].bar(comparison_df['Language_Type'], comparison_df['C/V_Ratio'], color='lightcoral')\n",
    "axes[0,1].set_title('Consonant/Vowel Ratio')\n",
    "axes[0,1].set_ylabel('C/V Ratio')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Feature complexity\n",
    "axes[1,0].bar(comparison_df['Language_Type'], comparison_df['Avg_Complexity'], color='lightgreen')\n",
    "axes[1,0].set_title('Average Feature Complexity')\n",
    "axes[1,0].set_ylabel('Features per Sound')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Rare features\n",
    "axes[1,1].bar(comparison_df['Language_Type'], comparison_df['Rare_Features'], color='gold')\n",
    "axes[1,1].set_title('Rare Feature Count')\n",
    "axes[1,1].set_ylabel('Number of Rare Features')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phonological Universals Testing\n",
    "\n",
    "Test various proposed phonological universals using the comprehensive Tresoldi system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_phonological_universals():\n",
    "    \"\"\"Test various phonological universal claims.\"\"\"\n",
    "    tresoldi_system = get_feature_system('tresoldi_distinctive')\n",
    "    \n",
    "    universals_tests = {}\n",
    "    \n",
    "    # Universal 1: If a language has /g/, it has /k/\n",
    "    def test_voicing_implication():\n",
    "        # This would need a database of language inventories\n",
    "        # For demo, test with our sample inventories\n",
    "        violations = 0\n",
    "        total_langs = 0\n",
    "        \n",
    "        for lang_type, inventory in language_inventories.items():\n",
    "            consonants = inventory['consonants']\n",
    "            has_g = 'g' in consonants\n",
    "            has_k = 'k' in consonants\n",
    "            \n",
    "            if has_g and not has_k:\n",
    "                violations += 1\n",
    "            total_langs += 1\n",
    "        \n",
    "        return {'violations': violations, 'total': total_langs, 'rate': violations/total_langs}\n",
    "    \n",
    "    # Universal 2: Voiceless stops are more common than voiced stops\n",
    "    def test_voicing_frequency():\n",
    "        voiceless_count = 0\n",
    "        voiced_count = 0\n",
    "        \n",
    "        stops = ['p', 'b', 't', 'd', 'k', 'g', 'q', 'ʔ']\n",
    "        \n",
    "        for stop in stops:\n",
    "            if tresoldi_system.has_grapheme(stop):\n",
    "                sound = Sound(stop, 'tresoldi_distinctive')\n",
    "                if (sound.get_feature_value('consonantal') > 0 and \n",
    "                    sound.get_feature_value('continuant') < 0):\n",
    "                    if sound.get_feature_value('voice') > 0:\n",
    "                        voiced_count += 1\n",
    "                    elif sound.get_feature_value('voice') < 0:\n",
    "                        voiceless_count += 1\n",
    "        \n",
    "        return {'voiceless': voiceless_count, 'voiced': voiced_count, 'ratio': voiceless_count/max(voiced_count,1)}\n",
    "    \n",
    "    # Universal 3: All languages have at least one nasal\n",
    "    def test_nasal_universal():\n",
    "        violations = 0\n",
    "        total_langs = 0\n",
    "        \n",
    "        for lang_type, inventory in language_inventories.items():\n",
    "            has_nasal = False\n",
    "            \n",
    "            for consonant in inventory['consonants']:\n",
    "                if tresoldi_system.has_grapheme(consonant):\n",
    "                    sound = Sound(consonant, 'tresoldi_distinctive')\n",
    "                    if sound.get_feature_value('nasal') > 0:\n",
    "                        has_nasal = True\n",
    "                        break\n",
    "            \n",
    "            if not has_nasal:\n",
    "                violations += 1\n",
    "            total_langs += 1\n",
    "        \n",
    "        return {'violations': violations, 'total': total_langs, 'rate': violations/total_langs}\n",
    "    \n",
    "    # Universal 4: If a language has clicks, it has other stops\n",
    "    def test_click_implication():\n",
    "        violations = 0\n",
    "        total_click_langs = 0\n",
    "        \n",
    "        for lang_type, inventory in language_inventories.items():\n",
    "            has_clicks = False\n",
    "            has_other_stops = False\n",
    "            \n",
    "            for consonant in inventory['consonants']:\n",
    "                if tresoldi_system.has_grapheme(consonant):\n",
    "                    sound = Sound(consonant, 'tresoldi_distinctive')\n",
    "                    \n",
    "                    if sound.has_feature('click') and sound.get_feature_value('click') > 0:\n",
    "                        has_clicks = True\n",
    "                    elif (sound.get_feature_value('consonantal') > 0 and \n",
    "                          sound.get_feature_value('continuant') < 0):\n",
    "                        has_other_stops = True\n",
    "            \n",
    "            if has_clicks:\n",
    "                total_click_langs += 1\n",
    "                if not has_other_stops:\n",
    "                    violations += 1\n",
    "        \n",
    "        return {'violations': violations, 'total': total_click_langs, 'rate': violations/max(total_click_langs,1)}\n",
    "    \n",
    "    # Run all tests\n",
    "    universals_tests['Voicing_Implication'] = test_voicing_implication()\n",
    "    universals_tests['Voicing_Frequency'] = test_voicing_frequency()\n",
    "    universals_tests['Nasal_Universal'] = test_nasal_universal()\n",
    "    universals_tests['Click_Implication'] = test_click_implication()\n",
    "    \n",
    "    return universals_tests\n",
    "\n",
    "universal_results = test_phonological_universals()\n",
    "\n",
    "print(\"Phonological Universals Testing:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"1. Voicing Implication (if /g/ then /k/):\")\n",
    "voicing_impl = universal_results['Voicing_Implication']\n",
    "print(f\"   Violations: {voicing_impl['violations']}/{voicing_impl['total']} ({voicing_impl['rate']:.1%})\")\n",
    "\n",
    "print(\"\\n2. Voicing Frequency (voiceless > voiced stops):\")\n",
    "voicing_freq = universal_results['Voicing_Frequency']\n",
    "print(f\"   Voiceless stops: {voicing_freq['voiceless']}\")\n",
    "print(f\"   Voiced stops: {voicing_freq['voiced']}\")\n",
    "print(f\"   Ratio: {voicing_freq['ratio']:.2f}\")\n",
    "\n",
    "print(\"\\n3. Nasal Universal (all languages have nasals):\")\n",
    "nasal_univ = universal_results['Nasal_Universal']\n",
    "print(f\"   Violations: {nasal_univ['violations']}/{nasal_univ['total']} ({nasal_univ['rate']:.1%})\")\n",
    "\n",
    "print(\"\\n4. Click Implication (if clicks then other stops):\")\n",
    "click_impl = universal_results['Click_Implication']\n",
    "print(f\"   Violations: {click_impl['violations']}/{click_impl['total']} ({click_impl['rate']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Gradient Sound Change Modeling\n",
    "\n",
    "Model realistic sound change with continuous feature values and variable application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_realistic_lenition():\n",
    "    \"\"\"Model realistic lenition process with gradient application.\"\"\"\n",
    "    engine = SoundChangeEngine(feature_system_name='unified_distinctive')\n",
    "    \n",
    "    # Progressive lenition stages\n",
    "    lenition_stages = [\n",
    "        # Stage 1: Initial weakening (20% progress)\n",
    "        FeatureChangeRule(\n",
    "            name=\"lenition_stage1\",\n",
    "            feature_conditions={\"consonantal\": 1.0, \"continuant\": -1.0, \"sonorant\": -1.0},\n",
    "            feature_changes=[\n",
    "                FeatureChange(\n",
    "                    feature_name=\"continuant\",\n",
    "                    target_value=0.2,\n",
    "                    change_type=ChangeType.GRADIENT,\n",
    "                    change_strength=0.3\n",
    "                )\n",
    "            ],\n",
    "            environment=PhonologicalEnvironment(left_pattern=\"V\", right_pattern=\"V\")\n",
    "        ),\n",
    "        \n",
    "        # Stage 2: Further weakening (50% progress)\n",
    "        FeatureChangeRule(\n",
    "            name=\"lenition_stage2\",\n",
    "            feature_conditions={\"consonantal\": 1.0, \"continuant\": 0.2},\n",
    "            feature_changes=[\n",
    "                FeatureChange(\n",
    "                    feature_name=\"continuant\",\n",
    "                    target_value=0.6,\n",
    "                    change_type=ChangeType.GRADIENT,\n",
    "                    change_strength=0.5\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        \n",
    "        # Stage 3: Complete fricativization (100% progress)\n",
    "        FeatureChangeRule(\n",
    "            name=\"lenition_stage3\",\n",
    "            feature_conditions={\"consonantal\": 1.0, \"continuant\": 0.6},\n",
    "            feature_changes=[\n",
    "                FeatureChange(\n",
    "                    feature_name=\"continuant\",\n",
    "                    target_value=1.0,\n",
    "                    change_type=ChangeType.GRADIENT,\n",
    "                    change_strength=0.8\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    lenition_process = RuleSet(rules=lenition_stages, iterative=True, max_iterations=5)\n",
    "    \n",
    "    # Test words with intervocalic stops\n",
    "    test_words = [\n",
    "        ['a', 'p', 'a'],\n",
    "        ['e', 't', 'a'],  \n",
    "        ['i', 'k', 'o']\n",
    "    ]\n",
    "    \n",
    "    print(\"Gradient Lenition Process:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for word in test_words:\n",
    "        sounds = [Sound(s, 'unified_distinctive') for s in word]\n",
    "        original = ' '.join([s.grapheme() for s in sounds])\n",
    "        \n",
    "        # Track lenition progress\n",
    "        current_sounds = sounds\n",
    "        print(f\"\\nWord: {original}\")\n",
    "        \n",
    "        # Apply lenition process\n",
    "        result = engine.apply_rule_set(lenition_process, sounds)\n",
    "        \n",
    "        # Show final state\n",
    "        final = ' '.join([s.grapheme() for s in result.final_sequence])\n",
    "        \n",
    "        # Show continuancy values for middle consonant\n",
    "        if len(result.final_sequence) >= 2:\n",
    "            middle_consonant = result.final_sequence[1]\n",
    "            continuant_val = middle_consonant.get_feature_value('continuant')\n",
    "            print(f\"  Final: {final} (continuant = {continuant_val:.2f})\")\n",
    "            \n",
    "            # Interpret gradient value\n",
    "            if continuant_val < 0.3:\n",
    "                interpretation = \"stop\"\n",
    "            elif continuant_val < 0.7:\n",
    "                interpretation = \"weakened stop / affricate\"\n",
    "            else:\n",
    "                interpretation = \"fricative\"\n",
    "            print(f\"  Interpretation: {interpretation}\")\n",
    "\n",
    "model_realistic_lenition()\n",
    "\n",
    "# Visualize lenition trajectory\n",
    "def plot_lenition_trajectory():\n",
    "    \"\"\"Plot the trajectory of lenition over time.\"\"\"\n",
    "    # Simulate lenition progress over time\n",
    "    time_steps = np.linspace(0, 100, 21)  # 0 to 100 years\n",
    "    continuant_values = []\n",
    "    \n",
    "    for t in time_steps:\n",
    "        # Sigmoid-like progression\n",
    "        progress = 1 / (1 + np.exp(-(t - 50) / 15))\n",
    "        continuant_val = -1.0 + 2.0 * progress  # Scale from -1 to +1\n",
    "        continuant_values.append(continuant_val)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_steps, continuant_values, 'b-', linewidth=2, label='Continuant feature value')\n",
    "    plt.axhline(y=-1, color='r', linestyle='--', alpha=0.5, label='Stop (-1.0)')\n",
    "    plt.axhline(y=0, color='orange', linestyle='--', alpha=0.5, label='Intermediate (0.0)')\n",
    "    plt.axhline(y=1, color='g', linestyle='--', alpha=0.5, label='Fricative (+1.0)')\n",
    "    \n",
    "    plt.xlabel('Time (arbitrary units)')\n",
    "    plt.ylabel('Continuant Feature Value')\n",
    "    plt.title('Gradient Lenition: Stop → Fricative Trajectory')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_lenition_trajectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "This notebook has demonstrated the full range of AlteruPhono's capabilities for historical linguistics research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of workflow capabilities\n",
    "print(\"AlteruPhono Historical Linguistics Workflow Summary\")\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "\n",
    "capabilities = [\n",
    "    \"✓ Comparative reconstruction from cognate sets\",\n",
    "    \"✓ Language evolution simulation with sound change rules\", \n",
    "    \"✓ Phonological distance calculation for classification\",\n",
    "    \"✓ Automatic sound change rule discovery\",\n",
    "    \"✓ Cross-linguistic typological analysis\",\n",
    "    \"✓ Phonological universals testing\",\n",
    "    \"✓ Gradient sound change modeling\",\n",
    "    \"✓ Feature-based phonological analysis\",\n",
    "    \"✓ Multi-system compatibility (IPA, Unified, Tresoldi)\",\n",
    "    \"✓ Large-scale cross-linguistic coverage (1,081 sounds)\"\n",
    "]\n",
    "\n",
    "for capability in capabilities:\n",
    "    print(capability)\n",
    "\n",
    "print(\"\\nKey Research Applications:\")\n",
    "print(\"-\" * 30)\n",
    "applications = [\n",
    "    \"• Comparative method automation\",\n",
    "    \"• Language family classification\", \n",
    "    \"• Sound change pathway modeling\",\n",
    "    \"• Phonological typology studies\",\n",
    "    \"• Historical phonology reconstruction\",\n",
    "    \"• Language contact analysis\",\n",
    "    \"• Evolutionary linguistics simulation\"\n",
    "]\n",
    "\n",
    "for application in applications:\n",
    "    print(application)\n",
    "\n",
    "print(\"\\nNext Steps for Research:\")\n",
    "print(\"-\" * 25)\n",
    "next_steps = [\n",
    "    \"1. Apply to larger datasets with real language families\",\n",
    "    \"2. Integrate with phylogenetic reconstruction methods\",\n",
    "    \"3. Develop machine learning models for change prediction\",\n",
    "    \"4. Create comprehensive typological databases\",\n",
    "    \"5. Model contact-induced changes and borrowing\",\n",
    "    \"6. Validate against known historical changes\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"AlteruPhono: Empowering Historical Linguistics Research\")\n",
    "print(\"=\" * 55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
